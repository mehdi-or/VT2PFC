{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdi-or/VT2PFC/blob/main/DATRACE_with_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD1uTW1uzP47",
        "outputId": "864ed574-6e3a-4ea1-85f5-8b1e1d29ed0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset\n",
        "import multiprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-SXgf8T0W6j"
      },
      "source": [
        "##finction to set the random see for reproducibility purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0_tKFUDd0Yin"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    random.seed(seed_value)  # Python random module\n",
        "    np.random.seed(seed_value)  # Numpy module\n",
        "    torch.manual_seed(seed_value)  # PyTorch random number generator for CPU\n",
        "\n",
        "    # If you are using CUDA\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
        "\n",
        "    # Additional configurations to enhance reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUWr1MPh0iE6"
      },
      "source": [
        "##setting up the architecture of the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LCNR9Jv50jPK"
      },
      "outputs": [],
      "source": [
        "class DATRACE(nn.Module):\n",
        "    def __init__(self, VTC_dim, PFC_dim, hidden_dim, bottleneck_dim, num_classes):\n",
        "        super(DATRACE, self).__init__()\n",
        "        # Encoding layers for the VTC\n",
        "        self.encoder_VTC = nn.Linear(VTC_dim, hidden_dim)\n",
        "\n",
        "        # Shared bottleneck layer\n",
        "        self.shared_bottleneck = nn.Linear(hidden_dim, bottleneck_dim)\n",
        "\n",
        "        # Decoding layers for PFC\n",
        "        self.decoder = nn.Linear(bottleneck_dim, hidden_dim)\n",
        "        self.prediction_PFC = nn.Linear(hidden_dim, PFC_dim)\n",
        "\n",
        "        # Classification layer attached to the shared bottleneck\n",
        "        self.classifier = nn.Linear(bottleneck_dim, num_classes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Encoder VTC\n",
        "        enocded_VTC = torch.tanh(self.encoder_VTC(x))\n",
        "        enocded_VTC = self.dropout(enocded_VTC)\n",
        "        BN_shared = torch.tanh(self.shared_bottleneck(enocded_VTC))\n",
        "        return BN_shared\n",
        "\n",
        "    def decode(self, x):\n",
        "        # Decoder PFC\n",
        "        decoded = torch.tanh(self.decoder(x))\n",
        "        decoded = self.dropout(decoded)\n",
        "        predicted_PFC = self.prediction_PFC(decoded)\n",
        "        return predicted_PFC\n",
        "\n",
        "    def logis(self, x):\n",
        "        # Classifier\n",
        "        logits = self.classifier(x)\n",
        "        probabilities = logits\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder VTC\n",
        "        BN = self.encode(x)\n",
        "        # Decoder PFC\n",
        "        predicted_PFC = self.decode(BN)\n",
        "        # Classifier\n",
        "        probabilities = self.logis(BN)\n",
        "\n",
        "        return predicted_PFC, probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sa5Dc1m0lQP"
      },
      "source": [
        "##importing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O6QXsIAt0mq6"
      },
      "outputs": [],
      "source": [
        "def load_data_VTC(subject):\n",
        "    with h5py.File(r'/content/gdrive/MyDrive/Colab Notebooks/CNC data/hrfAll_VT_PETERS.hdf5', 'r') as hdf:\n",
        "        data0 = hdf.get('items/'+str(subject)+'/rcargs/items/0')\n",
        "        data_vtc = np.array(data0)\n",
        "        data_vtc = np.delete(data_vtc,np.where(~data_vtc.any(axis=0))[0],axis=1)\n",
        "    return(data_vtc)\n",
        "\n",
        "def load_data_PFC(subject):\n",
        "    with h5py.File(r'/content/gdrive/MyDrive/Colab Notebooks/CNC data/hrfAll_DLPFC_PETERS.hdf5', 'r') as hdf:\n",
        "        data0_pfc = hdf.get('items/'+str(subject)+'/rcargs/items/0')\n",
        "        data_pfc = np.array(data0_pfc)\n",
        "        data_pfc = np.delete(data_pfc,np.where(~data_pfc.any(axis=0))[0],axis=1)\n",
        "    return(data_pfc)\n",
        "\n",
        "def preprocessign (data, labels2categ):\n",
        "  shuffle_index = np.arange(0,3600)\n",
        "  data_train, data_test, y_categ_train, y_categ_test, map_train_index, map_test_index = train_test_split(data, labels2categ, shuffle_index, random_state=42)\n",
        "  #scaler = StandardScaler()\n",
        "  scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "  X_train = scaler.fit_transform(data_train)\n",
        "  X_test = scaler.transform(data_test)\n",
        "  return X_train, X_test, y_categ_train, y_categ_test, map_train_index, map_test_index\n",
        "\n",
        "#setting the labels for pytorch is differen from keras\n",
        "# the way it works is that we need to assign a number to each categorical class\n",
        "unique_labels = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/CNC data/unique_aranged.csv', header=None).values[:,1]\n",
        "labels = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/CNC data/label.csv')['y'].values\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)} #mapping form label to its numeric value\n",
        "index_to_label = {idx: label for label, idx in label_to_index.items()} #mapping from numeric label to the name of the label\n",
        "\n",
        "#turning label file into its numeric values\n",
        "numeric_labels = []\n",
        "for label in labels:\n",
        "  numeric_labels.append(label_to_index[label])\n",
        "\n",
        "numeric_labels = np.array(numeric_labels)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#converting all the numpy array inot pytorch tensor\n",
        "\n",
        "# y_train and y_test is the same for every subject therefore\n",
        "VTC0 = load_data_VTC(37)\n",
        "_, _, y_train, y_test, map_train_index, map_test_index = preprocessign(VTC0, numeric_labels)\n",
        "y_tensor_train = torch.tensor(y_train).to(device)\n",
        "y_tensor_test =torch.tensor(y_test).to(device)\n",
        "trainidx = int(len(y_tensor_test)*0.8)\n",
        "y_tensor_test_train, y_tensor_test_test = y_tensor_test[:trainidx], y_tensor_test[trainidx:]\n",
        "\n",
        "def load_subject(subject):\n",
        "  VTC0 = load_data_VTC(subject)\n",
        "  #VTC0 = np.random.normal(0,1, (3600, 2450)) #giving random noise as VTC data\n",
        "  PFC0 = load_data_PFC(subject)\n",
        "  VTC_dim = VTC0.shape[1]\n",
        "  PFC_dim = PFC0.shape[1]\n",
        "  num_classes = len(np.unique(labels))\n",
        "  shuffle_index = np.arange(0,3600)\n",
        "  VTC_train, VTC_test, _, _, _, _ = preprocessign(VTC0, numeric_labels)\n",
        "  PFC_train, PFC_test, _, _, _, _ = preprocessign(PFC0, numeric_labels)\n",
        "\n",
        "  VTC_tensor_train = torch.tensor(VTC_train, dtype=torch.float32).to(device)\n",
        "  VTC_tensor_test = torch.tensor(VTC_test, dtype=torch.float32).to(device)\n",
        "  PFC_tensor_train = torch.tensor(PFC_train, dtype=torch.float32).to(device)\n",
        "  PFC_tensor_test = torch.tensor(PFC_test, dtype=torch.float32).to(device)\n",
        "\n",
        "  train_dataset = TensorDataset(VTC_tensor_train, PFC_tensor_train, y_tensor_train)\n",
        "  test_dataset = TensorDataset(VTC_tensor_test, PFC_tensor_test, y_tensor_test)\n",
        "  return train_dataset, test_dataset, VTC_tensor_test, PFC_tensor_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKXzhLbt0oxY"
      },
      "source": [
        "##Function for calculating the metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UdbDPbEr0p03"
      },
      "outputs": [],
      "source": [
        "def fidelity(x, x_pred):\n",
        "  x = x.detach().cpu().numpy()\n",
        "  #x_pred = x_pred.cpu().numpy()\n",
        "  #calculating recons fidelity\n",
        "  corr_x = np.corrcoef(x, x_pred)[:x.shape[0],x.shape[0]:]\n",
        "  corr_x = np.diag(corr_x)\n",
        "  recons_fidelity = np.mean(corr_x)\n",
        "  return recons_fidelity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23f1P5q0rjB"
      },
      "source": [
        "## function for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rTeKcWIk0s9y"
      },
      "outputs": [],
      "source": [
        "#train and val the network\n",
        "def train_and_val_network (train_loader, val_loader, num_classes=40, BN_dim=30, num_epochs=300, alpha=0.1):\n",
        "  data_iterator = iter(val_loader)\n",
        "  input_VTC, input_PFC, labels = next(data_iterator)\n",
        "  VTC_dim = input_VTC.shape[1]\n",
        "  PFC_dim = input_PFC.shape[1]\n",
        "  input_PFC = input_PFC.shape[1]\n",
        "  # Define the model, optimizer, and loss functions\n",
        "  model = DATRACE(VTC_dim=VTC_dim, PFC_dim=PFC_dim, hidden_dim=500, bottleneck_dim=BN_dim, num_classes=num_classes).to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "  #mse_loss_fn = nn.MSELoss()\n",
        "  mse_loss_fn = nn.L1Loss()\n",
        "  #mse_loss_fn = CorrelatonLoss()\n",
        "  classification_loss_fn = nn.CrossEntropyLoss()\n",
        "  train_loss_PFC_hist = []\n",
        "  train_loss_class_hist = []\n",
        "  val_PFC_loss_hist = []\n",
        "  train_loss_hist = []\n",
        "  val_loss_hist = []\n",
        "  for epoch in range(num_epochs):\n",
        "      train_loss = 0\n",
        "      predicted_loss_PFC = 0\n",
        "      classification_loss = 0\n",
        "      set_seed(42 + epoch)\n",
        "      model.train()\n",
        "      for input_VTC, input_PFC, labels in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          predicted_PFC, probabilities = model(input_VTC)\n",
        "          # Calculate losses\n",
        "          predicted_loss_PFC0 = mse_loss_fn(predicted_PFC, input_PFC)\n",
        "          classification_loss0 = classification_loss_fn(probabilities, labels)\n",
        "          # Total loss\n",
        "          total_loss = predicted_loss_PFC0 + alpha*classification_loss0\n",
        "          # Backpropagation and optimizer step\n",
        "          total_loss.backward()\n",
        "          optimizer.step()\n",
        "          predicted_loss_PFC += predicted_loss_PFC0.item()\n",
        "          classification_loss += classification_loss0.item()\n",
        "          train_loss += total_loss.item()\n",
        "      predicted_loss_PFC /= len(train_loader)\n",
        "      classification_loss /= len(train_loader)\n",
        "      train_loss /= len(train_loader)\n",
        "      train_loss_PFC_hist.append(predicted_loss_PFC)\n",
        "      train_loss_class_hist.append(classification_loss)\n",
        "      train_loss_hist.append(train_loss)\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      val_loss = 0\n",
        "      val_PFC_loss = 0\n",
        "      with torch.no_grad():\n",
        "          for input_VTC, input_PFC, labels in val_loader:\n",
        "              reconstructed, probabilities = model(input_VTC)\n",
        "              PFC_loss = mse_loss_fn(reconstructed, input_PFC)\n",
        "              classification_loss = classification_loss_fn(probabilities, labels)\n",
        "              loss = PFC_loss + alpha*classification_loss\n",
        "              val_PFC_loss += PFC_loss.item()\n",
        "              val_loss += loss.item()\n",
        "\n",
        "      val_PFC_loss /= len(val_loader)\n",
        "      val_loss /= len(val_loader)\n",
        "      val_PFC_loss_hist.append(val_PFC_loss)\n",
        "      val_loss_hist.append(val_loss)\n",
        "\n",
        "      if (epoch+1)%(num_epochs/10)==0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} completed for alpha={alpha}.\")\n",
        "  return model, train_loss_hist, val_loss_hist\n",
        "\n",
        "\n",
        "def subject_run(subject, ytrain=y_tensor_test_train, ytest=y_tensor_test_test):\n",
        "  # writting a for loop to run the model for different bottleneck dimensions\n",
        "  # And calculating the metrics we want\n",
        "  train_dataset, test_dataset, VTC_tensor_test, PFC_tensor_test= load_subject(subject)\n",
        "\n",
        "  set_seed(42) # to get the same training and testset everytime\n",
        "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "  val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "  data_iterator = iter(val_loader)\n",
        "  input_VTC, input_PFC, labels = next(data_iterator)\n",
        "  VTC_dim = input_VTC.shape[1]\n",
        "  PFC_dim = input_PFC.shape[1]\n",
        "  input_PFC = input_PFC.shape[1]\n",
        "\n",
        "  alpha = 0.1 # Classifier weigth of the network\n",
        "  num_epochs = 300\n",
        "  num_classes = 40\n",
        "  BN_dims = [10, 20, 30, 50, 100, 250, 500]\n",
        "  accuracies = []\n",
        "  pfc_fidels = []\n",
        "  for BN_dim in BN_dims:\n",
        "    model, train_loss_hist, val_loss_hist = train_and_val_network(train_loader, val_loader, num_classes, BN_dim, num_epochs, alpha)\n",
        "    # Fetch a batch of test images\n",
        "    predicted_pfc, pred_labels = model(VTC_tensor_test)\n",
        "    predicted_pfc = predicted_pfc.detach().cpu().numpy()\n",
        "\n",
        "    #fidelity\n",
        "    pfc_fidel = fidelity(PFC_tensor_test, predicted_pfc)\n",
        "    pfc_fidels.append(pfc_fidel)\n",
        "\n",
        "    # training the seperate classifier\n",
        "    X_train2, X_test2, y_train2, y_test2= train_test_split(predicted_pfc, y_test, random_state=42)\n",
        "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
        "    model.fit(X_train2, y_train2)\n",
        "    y_test_pred2 = model.predict(X_test2)\n",
        "    # Calculate the accuracy on the test set\n",
        "    test_accuracy = accuracy_score(y_test2, y_test_pred2)\n",
        "    accuracies.append(test_accuracy)\n",
        "\n",
        "    #saving the data\n",
        "  column_labels = np.array(['dim', 'fidelity', 'prediction classifier accuracy'])\n",
        "  all_data = np.array(np.column_stack((BN_dims, pfc_fidels, accuracies)))\n",
        "  all_data_df = pd.DataFrame(all_data, columns=column_labels)\n",
        "  all_data_df.to_csv('/content/gdrive/MyDrive/Colab Notebooks/VT2PFC/VTC2PFC/defense/DATRACE_subject_' + str(subject) + '.csv', index=False)\n",
        "  return all_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q2741cR9emW"
      },
      "outputs": [],
      "source": [
        "subjects = list(range(50, 71))\n",
        "for subject in subjects:\n",
        "  all_data_df = subject_run(subject, ytrain=y_tensor_test_train, ytest=y_tensor_test_test)\n",
        "  all_data_df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPbRKxCQ0cdGdG6AefcFFo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}