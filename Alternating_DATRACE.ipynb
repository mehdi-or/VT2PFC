{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNShFJ6FdVZ2NzU4HNnzgJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdi-or/VT2PFC/blob/main/Alternating_DATRACE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQaaLOZCZaAU"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for setting seed for reproducility"
      ],
      "metadata": {
        "id": "34eaW-icZk8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    random.seed(seed_value)  # Python random module\n",
        "    np.random.seed(seed_value)  # Numpy module\n",
        "    torch.manual_seed(seed_value)  # PyTorch random number generator for CPU\n",
        "\n",
        "    # If you are using CUDA\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
        "\n",
        "    # Additional configurations to enhance reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "ibjoKU_9ZmEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Implimentation of DATRACE (VT2PF)"
      ],
      "metadata": {
        "id": "sR5xtc3vZpXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DATRACE(nn.Module):\n",
        "    def __init__(self, input_size_VT, input_size_PF, hidden_size, bottleneck_size, num_classes):\n",
        "        super(DATRACE, self).__init__()\n",
        "        # Encoder\n",
        "        self.VT_in = nn.Linear(input_size_VT, hidden_size)\n",
        "        self.PF_in = nn.Linear(input_size_PF, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, bottleneck_size)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(bottleneck_size, hidden_size)\n",
        "        self.PF_out = nn.Linear(hidden_size, input_size_PF)\n",
        "        self.VT_out = nn.Linear(hidden_size, input_size_VT)\n",
        "\n",
        "        # Classifier connected to the bottleneck\n",
        "        self.classifier = nn.Linear(bottleneck_size, num_classes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Encoder 1\n",
        "        x1 = torch.tanh(self.VT_in(x1))\n",
        "        x1 = self.dropout(x1)\n",
        "        encoded = torch.tanh(self.fc2(x1))\n",
        "\n",
        "        #Encoder 2\n",
        "        x2 = torch.tanh(self.VT_in(x2))\n",
        "        x2 = self.dropout(x2)\n",
        "        encoded = torch.tanh(self.fc2(x2)) # here the \"encoded\" is simply just overwritten by \"fc2(x2)\". This is a problem that requires to concatenate the ouput of fc2(x1) and fc2(x2)\n",
        "\n",
        "        # Decoder 1\n",
        "        x1 = torch.tanh(self.fc3(encoded))\n",
        "        decoded_PF = self.PF_out(x1)\n",
        "\n",
        "        # Decoder 2\n",
        "        x2 = torch.tanh(self.fc3(encoded))\n",
        "        decoded_VT = self.VT_out(x2)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.classifier(encoded)\n",
        "        #probabilities = F.softmax(logits, dim=1)\n",
        "        probabilities = logits\n",
        "\n",
        "        return decoded_PF, decoded_VT, probabilities\n",
        "\n",
        "\n",
        "# Example model instantiation\n",
        "input_size_VT = 1500 # e.g., for MNIST\n",
        "input_size_PF = 1000\n",
        "hidden_size = 128\n",
        "bottleneck_size = 32\n",
        "num_classes = 10 # e.g., for MNIST classification\n",
        "\n",
        "model = DATRACE(input_size_VT=input_size_VT, input_size_PF=input_size_PF, hidden_size=hidden_size,\n",
        "                                  bottleneck_size=bottleneck_size, num_classes=num_classes)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "sJWeg7UsZpFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT implementation of alternating architecture"
      ],
      "metadata": {
        "id": "yVnogIY3ZtbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DATRACE(nn.Module):\n",
        "    def __init__(self, input_dim_VT, input_dim_PF, hidden_dim, bottleneck_dim, num_classes):\n",
        "        super(DATRACE, self).__init__()\n",
        "        # Encoding layers for the VTC\n",
        "        self.encoder_a = nn.Linear(input_dim_VT, hidden_dim)\n",
        "        # Encoding layers for PFC\n",
        "        self.encoder_b = nn.Linear(input_dim_PF, hidden_dim)\n",
        "\n",
        "        # Shared bottleneck layer\n",
        "        self.shared_bottleneck = nn.Linear(hidden_dim, bottleneck_dim)  # Assume same dim for simplicity\n",
        "\n",
        "        # Decoding layers for PFC\n",
        "        self.decoder_a = nn.Linear(bottleneck_dim, hidden_dim)\n",
        "        self.prediction_PF = nn.Linear(hidden_dim, input_dim_PF)\n",
        "\n",
        "        # Decoding layers for VTC\n",
        "        self.decoder_b = nn.Linear(bottleneck_dim, hidden_dim)\n",
        "        self.prediction_VT = nn.Linear(hidden_dim, input_dim_VT)\n",
        "\n",
        "        # Classification layer attached to the shared bottleneck\n",
        "        self.classifier = nn.Linear(bottleneck_dim, num_classes)\n",
        "\n",
        "    def forward_VT2PF(self, x_a):\n",
        "        encoded_a = torch.tanh(self.encoder_a(x_a))\n",
        "        bottleneck = torch.tanh(self.shared_bottleneck(encoded_a))\n",
        "        decoded_a = torch.tanh(self.decoder_a(bottleneck))\n",
        "        predicted_PF = self.prediction_PF(decoded_a)\n",
        "        class_logits = self.classifier(bottleneck)\n",
        "        return predicted_PF, class_logits\n",
        "\n",
        "    def forward_PF2VT(self, x_b):\n",
        "        encoded_b = torch.tanh(self.encoder_b(x_b))\n",
        "        bottleneck = torch.tanh(self.shared_bottleneck(encoded_b))\n",
        "        decoded_b = torch.tanh(self.decoder_b(bottleneck))\n",
        "        predicted_VT = self.prediction_VT(decoded_b)\n",
        "        class_logits = self.classifier(bottleneck)\n",
        "        return predicted_VT, class_logits\n",
        "\n",
        "# Example usage\n",
        "input_dim_a = 784  # Example dimensions for different inputs\n",
        "input_dim_b = 512\n",
        "hidden_dim = 128  # Set equal for simplicity\n",
        "bottleneck_dim = 32\n",
        "num_classes = 10  # For multi-class classification\n",
        "\n",
        "model = DATRACE(input_dim_a, input_dim_b, hidden_dim, bottleneck_dim, num_classes)\n",
        "\n",
        "# Example inputs\n",
        "x_a = torch.randn(10, input_dim_a)  # Batch of inputs for autoencoder A\n",
        "x_b = torch.randn(10, input_dim_b)  # Batch of inputs for autoencoder B\n",
        "\n",
        "# Forward passes\n",
        "reconstructed_a, class_logits_a = model.forward_VT2PF(x_a)\n",
        "reconstructed_b, class_logits_b = model.forward_PF2VT(x_b)\n",
        "\n",
        "print(f\"Reconstruction A Shape: {reconstructed_a.shape}, Classification Logits A Shape: {class_logits_a.shape}\")\n",
        "print(f\"Reconstruction B Shape: {reconstructed_b.shape}, Classification Logits B Shape: {class_logits_b.shape}\")\n"
      ],
      "metadata": {
        "id": "0nvmf6ZRZt6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT alternating Training"
      ],
      "metadata": {
        "id": "07s4t25WZ4Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume DataLoader for autoencoder A and B are defined as data_loader_a and data_loader_b\n",
        "\n",
        "# Model, Loss Functions, and Optimizer\n",
        "model = DATRACE(input_dim_a, input_dim_b, hidden_dim, bottleneck_dim, num_classes)\n",
        "mse_loss_fn = nn.MSELoss()\n",
        "cross_entropy_loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example DataLoaders (Replace with your actual DataLoader)\n",
        "# data_loader_a = DataLoader(...)\n",
        "# data_loader_b = DataLoader(...)\n",
        "\n",
        "def train_autoencoder(autoencoder, data_loader, update_classifier=False):\n",
        "    \"\"\"\n",
        "    Trains either autoencoder A or B, based on the function passed in `autoencoder`.\n",
        "    If `update_classifier` is True, also updates the classifier.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for inputs, labels in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed, class_logits = autoencoder(inputs)\n",
        "\n",
        "        # Calculate losses\n",
        "        reconstruction_loss_VT = mse_loss_fn(reconstructed, inputs)\n",
        "        classification_loss = cross_entropy_loss_fn(class_logits, labels)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = reconstruction_loss\n",
        "        if update_classifier:\n",
        "            total_loss += classification_loss\n",
        "\n",
        "        # Backpropagation and optimizer step\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 5  # Example setting\n",
        "for epoch in range(num_epochs):\n",
        "    # Alternate training between autoencoders and classifier\n",
        "    if epoch % 2 == 0:\n",
        "        # Freeze parameters for autoencoder B\n",
        "        for param in model.encoder_b.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in model.decoder_b.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze and train autoencoder A\n",
        "        for param in model.encoder_a.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in model.decoder_a.parameters():\n",
        "            param.requires_grad = True\n",
        "        train_autoencoder(model.forward_autoencoder_a, data_loader_a, update_classifier=True)\n",
        "    else:\n",
        "        # Freeze parameters for autoencoder A\n",
        "        for param in model.encoder_a.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in model.decoder_a.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze and train autoencoder B\n",
        "        for param in model.encoder_b.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in model.decoder_b.parameters():\n",
        "            param.requires_grad = True\n",
        "        train_autoencoder(model.forward_autoencoder_b, data_loader_b, update_classifier=True)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n"
      ],
      "metadata": {
        "id": "nBJwrz_VZ6Cq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}